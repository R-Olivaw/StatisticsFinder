#------------------------------------------------------------------------------
'''
These modules are for summarizing the article.
'''
from __future__ import absolute_import
from __future__ import division, print_function, unicode_literals
from sumy.parsers.html import HtmlParser
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer as Summarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

#------------------------------------------------------------------------------

class ScraperTool():
    
    def __init__(self, url_list):
        #This is the .txt file generated by the searcher module.
        self.url_list = url_list
        self.url_snippet = None
        self.sum_collection = None
        #This is the top most url in the .txt file. It will get deleted.
        self.current_url = None
        #This is a string version of the name of the file containing the fetched headers.
        self.output_file_id = None
        #The percentage document
        self.percent_doc_string = None
        #A temporary string
        self.temp_string = None
        self.cnt = 0
        self.pcnt = 0
        self.url_total = 0
    
    #This function grabs a block of text - usually an hmtl <p> tag - and separates each sentence with a newline.
    #The function passes the separated sentences to the temp_string variable.
    def split_paragraphs(self, string):
        import re
        periods = re.sub(r'\. ', r'. \n\n', string)
        questions = re.sub(r'\? ', r'? \n\n', periods)
        exclamations = re.sub(r'\! ', r'! \n\n', questions)
    
        self.temp_string = exclamations
        
    
    #This function searches for all header tags and writes them to a new txt file.
    def grab_headers_and_statistics(self, folder):
        import requests, re, os
        requests.packages.urllib3.disable_warnings() 
        from bs4 import BeautifulSoup as bs
        
        #This probably isn't necessary.
        headers = {
        'User-Agent': 'Gerald Fitzpatrick, thefitzpatrickendeavor.com',
        'From:': 'CEO@thefitzpatrickendeavor.com'
        }

        with open(self.url_list, 'r+', encoding='utf-8') as list_doc:
            pos = 0 
            for line in list_doc:
                self.url_total += 1
                pos += 1
                #This chunk uses a regex to create a shortened version of the url.
                #The if statement checks for www and passes the url_snippet to the appropriate part of the string.
                wwwRegex = re.compile(r'www')
                if not wwwRegex.search(line):
                    #If there is no www, we use this regex.
                    noWwwGrabRegex = re.compile(r'(//)(.+)(\.)')
                    self.url_snippet = noWwwGrabRegex.search(line).group(2)
                    self.url_snippet = self.url_snippet.replace('-', '')
        
                else:
                    #If there is a www, we use this regex.
                    shortRegex = re.compile(r'(\.)(.+)(\.)')
                    self.url_snippet = shortRegex.search(line).group(2)
                    self.url_snippet = self.url_snippet.replace('-', '')
                #--------------------------------------------------------------

                #Creates a file based on the url    
                file_name = (folder+"\\"+"{}. ".format(pos)+self.url_snippet.replace(r'/', '')+"_headers.txt")
                #replaces any " in the file name.
                file_name = file_name.replace(r'"', '')
                os.makedirs(os.path.dirname(file_name), exist_ok=True)
                #--------------------------------------------------------------
                
                #Links to google support pages don't format well.
                #This chunk checks for google support links and re-phrases them for file extensions.
                googleRegex = re.compile(r'google')
                google_count = 0
                if googleRegex.search(file_name):
                    google_count += 1
                    file_name = (folder+"\\"+"{}. ".format(pos)+"Google_link_{}".format(google_count)+"_headers.txt")
                #--------------------------------------------------------------
                
                #Requests the url and creates soup.        
                '''
                --important--
                The line.strip() here is very important. Without it, this entire loop will throw 400 and 404 errors for half the urls...
                --important--
                '''
                try:
                    page = requests.get(line.strip())
                #This exception is here because a ContentDecodingError will destroy everything. This just skips past the offending url.
                except requests.exceptions.ContentDecodingError:
                    print("ContentDecoding Error encountered! Skipped url no. {}!".format(self.cnt))
                    print("---")
                    continue
                else:
                    soup = bs(page.text, 'html.parser')
                #--------------------------------------------------------------
                
                with open(self.percent_doc_string, "a") as f:
                    
                    f.write("\n----------\n\n>>URL: " + line + "\n" )
                
                    #This chunk checks the webpage for 'percent' or '%', then writes the containing paragraphs to out file.
                    percentRegex = re.compile('(p)?ercent')
                    percentSymbolRegex = re.compile('\%')
            
                    for p in soup.find_all('p'):
                        mo1 = percentRegex.search(p.text)
                        mo2 = percentSymbolRegex.search(p.text)
                        try:
                            if mo1:
                                self.split_paragraphs(p.text)
                                f.write(self.temp_string+'\n')
                                self.pcnt += 1
                            elif mo2:
                                self.split_paragraphs(p.text)
                                f.write(self.temp_string+'\n')
                                self.pcnt += 1
                            continue
                        except:
                            pass
                    
                    #Once we write down any statistics, we include a summary of the article.
                    
                    f.write("\n-----SUMMARY-----\n\n")
                    
                    #This establishes the language of the articles we're reading.
                    LANGUAGE = "english"
                    #This is the maximum length of the summary.
                    SENTENCES_COUNT = 15
                    try:
                        #See SUMY documentation for explanation.
                        parser = HtmlParser.from_url(line.strip(), Tokenizer(LANGUAGE))

                        stemmer = Stemmer(LANGUAGE)
                    
                        summarizer = Summarizer(stemmer)
                        summarizer.stop_words = get_stop_words(LANGUAGE)
                    
                        #We split up the sentences and write them to the output file.
                        for sentence in summarizer(parser.document, SENTENCES_COUNT):
                            self.split_paragraphs(str(sentence))
                            f.write(self.temp_string+'\n')
                    
                    except requests.models.HTTPError:
                        print("SUMY Encountered a 404 error!")
                    except:
                        pass
                        
                        
                        
                #--------------------------------------------------------------
                f.close()
                
                
                with open(file_name, "w") as f:
                    try:
                        f.write("\n----------------------------------------------------\n                    HEADERS BELOW\n----------------------------------------------------\n")
                        
                        #This chunk searches for all header tags uing a regex, then writes them to the file.
                        headings_list = soup.find_all(re.compile(r'h\d+'))
                        
                        for item in headings_list:
                            header = item.get_text()
                            if isinstance(header, str):
                                #Not sure why, but some headers throw a UnicodeEncodeError and crash the whole thing.
                                #This try/except block skips over offending lines.
                                try:
                                    f.write(header+"\n")
                                except UnicodeEncodeError:
                                    print("Unicode Error Encounter! Skipped url no. {}!".format(self.cnt))
                                    print("---")
                                    pass
                    except FileNotFoundError:
                        print("FileNotFoundError in url no. {}!".format(self.cnt))
                        print("---")
                        continue
                            
                f.close()
                #--------------------------------------------------------------
                
                
                '''
                #Removes blank lines in the txt file.
                f = open((file_name), 'r+')
                d = f.readlines()
                f.seek(0)
                for i in d:
                    if not i.startswith('/') and i != '\n':
                        f.write(i)
                f.truncate()
                f.close()
                '''
                self.output_file_id = file_name
                
                self.cnt += 1
                print("Grabbed headers and statistics from url no. {}!".format(self.cnt))
                print("---")
    
    #This function creates the percent doc.            
    def build_percent_doc(self, folder):
        self.percent_doc_string = (folder+"\\"+"PERCENT_FIGURES.txt")
        #Probably redundant...but this ensures the folder title doesn't contain quotation marks.
        self.percent_doc_string = self.percent_doc_string.replace("\"","")
        f = open(self.percent_doc_string,"w+")
        f.write("""
                \n----------THIS DOCUMENT CONTAINS STATISTICS!----------\n
                """)
        f.close()
    
    #This function runs through each url and creates a summary of the article content.    
    def engage_supersummarizer_phase_one(self, folder):        
        LANGUAGE = "english"
        #This is the length of the summary output. May need some adjusting.
        SENTENCES_COUNT = 15
        
        #We open up the url list document.
        with open(self.url_list, 'r+', encoding='utf-8') as list_doc:
            for line in list_doc:
                
                #Noe the .strip() method here. Without each, many urls with turn up 404 errors.
                parser = HtmlParser.from_url(line.strip(), Tokenizer(LANGUAGE))

                stemmer = Stemmer(LANGUAGE)
                
                summarizer = Summarizer(stemmer)
                summarizer.stop_words = get_stop_words(LANGUAGE)
                
                #We create a string that is the intended directory information of our collected summaries.
                #Probably doesn't need to be run in the loop...but it works...
                self.sum_collection = (folder+"\\"+"Summary Collection.txt")
                
                #We create or open a text doc where we will store the collected summaries.
                with open(self.sum_collection, "a") as f:
                
                    for sentence in summarizer(parser.document, SENTENCES_COUNT):
                        #We call the split_paragraphs function to make the summaries easier to read.
                        self.split_paragraphs(str(sentence))
                        f.write(self.temp_string+'\n')
                        
    #This function runs through our document of collected summaries and creates a meta summary.                    
    def engage_supersummarizer_phase_two(self, folder):
        
        LANGUAGE = "english"
        #This is the length of the summary output. May need some adjusting.
        SENTENCES_COUNT = 20
        
        #We assign a local variable based on the summary collection string representing the file directory of the summary collection.
        meta_sum_file = self.sum_collection
        
        #Note we're using SUMY's plaintext parser.
        parser = PlaintextParser.from_file(meta_sum_file, Tokenizer(LANGUAGE))

        stemmer = Stemmer(LANGUAGE)
        
        summarizer = Summarizer(stemmer)
        summarizer.stop_words = get_stop_words(LANGUAGE)
        
        meta_sum = (folder+"\\"+"Meta Summary.txt")
        
        #We create or write to a text doc containing the meta summary.
        with open(meta_sum, "a") as f:
        
            for sentence in summarizer(parser.document, SENTENCES_COUNT):
                self.split_paragraphs(str(sentence))
                f.write(self.temp_string+'\n\n')
        

            
    def scrape(self, folder):
        import os
        #We create the doc for statistics data.
        self.build_percent_doc(folder)
        #We pull the headers and statistics from each url and write them to the percent doc.
        self.grab_headers_and_statistics(folder)
        #We summarize each article and write to the percent doc.
        self.engage_supersummarizer_phase_one(folder)
        #We create a meta summary of summaries.
        self.engage_supersummarizer_phase_two(folder)
        print("{} of {} urls sucessful!".format(self.cnt, self.url_total))
        print("Found {} potential statistics!".format(self.pcnt))
        print("Finished!")
        os.startfile(folder)



